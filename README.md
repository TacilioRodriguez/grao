# Projeto de ETL de Log√≠stica com PySpark e Arquitetura Medallion

Este projeto demonstra um pipeline de dados completo (ETL) constru√≠do no ambiente Databricks utilizando PySpark. O objetivo √© processar dados brutos de envios log√≠sticos, estrutur√°-los seguindo as melhores pr√°ticas da Arquitetura Medallion (Bronze, Silver e Gold) e, ao final, gerar tabelas agregadas com insights de neg√≥cio.

## üöÄ Tecnologias Utilizadas

* **Cloud:** Databricks
* **Processamento de Dados:** Apache Spark (com a API PySpark)
* **Formato de Armazenamento:** Delta Lake
* **Arquitetura:** Medallion (Bronze, Silver e Gold)
* **Linguagem:** Python

## üèõÔ∏è Arquitetura do Projeto

O pipeline foi estruturado em tr√™s camadas l√≥gicas, garantindo governan√ßa, qualidade e escalabilidade dos dados.

### ü•â Camada Bronze
A primeira camada armazena os dados brutos, exatamente como foram recebidos, servindo como uma fonte de verdade imut√°vel.
* **Tabela:** `grao.bronze.grain_shipping`
    * **Origem:** C√≥pia fiel dos dados do arquivo `grain_logistic_shipping.csv`.
* **Tabela:** `grao.bronze.calendario`
    * **Origem:** Tabela de dimens√£o de tempo, gerada para cobrir o per√≠odo de an√°lise e facilitar a manipula√ß√£o de datas.

### ü•à Camada Silver
Nesta camada, os dados brutos s√£o limpos, validados, enriquecidos e modelados em um esquema **Star Schema**, otimizado para consultas e an√°lises.

* **Tabelas de Dimens√£o:**
    * `dim_localizacao`: Cont√©m a lista de estados de destino dos envios.
    * `dim_cliente`: Descreve caracter√≠sticas dos clientes (neste caso, por g√™nero).
    * `dim_metodo_envio`: Detalha os atributos da opera√ß√£o de envio, como m√©todo, corredor de armazenagem e import√¢ncia.
    * `dim_calendario`: A tabela da camada Bronze √© utilizada como dimens√£o de tempo.

* **Tabela Fato:**
    * `fato_envios`: Tabela central que cont√©m as m√©tricas de cada envio (pre√ßo, peso, quantidade de itens, etc.) e as chaves estrangeiras (IDs) que a conectam a todas as tabelas de dimens√£o.

### ü•á Camada Gold
A camada final, constru√≠da a partir da Silver. Ela cont√©m dados agregados e prontos para o consumo por ferramentas de BI, dashboards e an√°lises de neg√≥cio.

* **Tabelas Agregadas:**
    * `gold_vendas_por_destino`: Sumariza a receita total, o n√∫mero de envios e o valor m√©dio por estado.
    * `gold_performance_por_metodo_envio`: Analisa a efici√™ncia de cada m√©todo de envio, calculando a taxa de entrega no prazo e a avalia√ß√£o m√©dia dos clientes.
    * `gold_resumo_mensal_operacoes`: Apresenta uma vis√£o mensal da evolu√ß√£o da receita, do volume de envios e do peso total transportado.

## üåä Fluxo do Pipeline de Dados

O processo de transforma√ß√£o dos dados seguiu as seguintes etapas:

1.  **Ingest√£o (Bronze):** Os dados brutos do CSV foram carregados e salvos como uma tabela Delta na camada Bronze, garantindo um ponto de partida confi√°vel para todo o pipeline.

2.  **Limpeza e Prepara√ß√£o:** Os nomes das colunas foram padronizados (removendo acentos, caracteres especiais e convertendo para min√∫sculas) para facilitar a manipula√ß√£o no PySpark.

3.  **Cria√ß√£o da Camada Silver:**
    * As tabelas de dimens√£o foram criadas extraindo-se os valores distintos das colunas categ√≥ricas da tabela Bronze e adicionando uma chave substituta (ID) para cada uma.
    * O desafio de converter as datas em formato de texto complexo (ex: "quarta-feira, 1 de junho de 2022") foi resolvido de forma robusta, utilizando um `JOIN` com a tabela `dim_calendario`.
    * A tabela `fato_envios` foi constru√≠da unindo a tabela Bronze com todas as dimens√µes, substituindo os valores de texto pelas suas respectivas chaves (IDs).

4.  **Cria√ß√£o da Camada Gold:**
    * A partir da uni√£o da tabela `fato_envios` com as dimens√µes, foram aplicadas opera√ß√µes de `groupBy` e agrega√ß√µes (`sum`, `avg`, `count`) para calcular as m√©tricas de neg√≥cio e criar as tabelas da camada Gold.
    * As colunas de valor foram arredondadas para duas casas decimais para uma apresenta√ß√£o mais limpa.

## üìä Insights Gerados

As tabelas da camada Gold foram projetadas para responder a perguntas de neg√≥cio cruciais, tais como:
* Quais s√£o os estados mais e menos lucrativos?
* Qual parceiro log√≠stico oferece o servi√ßo mais pontual e com maior satisfa√ß√£o do cliente?
* O volume de vendas est√° crescendo ou diminuindo ao longo dos meses?
* Existe alguma correla√ß√£o entre o m√©todo de envio e a avalia√ß√£o do cliente?

## üßë‚Äçüíª Como Executar

Para replicar este projeto, √© necess√°rio um ambiente Databricks configurado. O pipeline pode ser executado atrav√©s de notebooks, seguindo a ordem l√≥gica das camadas:
1.  **Notebook 01_Bronze:** Ingest√£o do CSV e cria√ß√£o da tabela `grao.bronze.grain_shipping`. Cria√ß√£o da tabela `grao.bronze.calendario`.
2.  **Notebook 02_Silver:** Leitura das tabelas Bronze e cria√ß√£o de todas as tabelas de dimens√£o e da tabela fato.
3.  **Notebook 03_Gold:** Leitura das tabelas Silver e cria√ß√£o das tabelas agregadas para an√°lise.

---

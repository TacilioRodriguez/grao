{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "302a0619-e328-43b8-bcb6-291bf0d922d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install great_expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d08c197-720a-4a8e-a450-54eded73efaa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libs"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "import great_expectations as gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d80230fc-7421-4af9-8fe5-3b158a2d88cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Lê arquivo csv"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"/Volumes/workspace/default/grain_logistic_shipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85756437-f916-4a7b-ba8f-a80a2303ae2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source_name = \"data-source-example\"\n",
    "data_source = context.data_sources.add_spark(name=data_source_name, persist=False)\n",
    "\n",
    "data_asset_name = \"example_data_asset\"\n",
    "data_asset = data_source.add_dataframe_asset(name=data_asset_name)\n",
    "\n",
    "batch_definition_name = \"example_batch_definition\"\n",
    "batch_definition = data_asset.add_batch_definition_whole_dataframe(\n",
    "    batch_definition_name\n",
    ")\n",
    "\n",
    "# Passamos o DataFrame que queremos validar para obter um \"Batch\" de dados\n",
    "batch_parameters = {\"dataframe\": df}\n",
    "batch = batch_definition.get_batch(batch_parameters=batch_parameters)\n",
    "\n",
    "print(\"Batch de dados criado. Iniciando validações...\")\n",
    "\n",
    "print(\"\\n--- Validando Nulos ---\")\n",
    "for col_name in df.columns:\n",
    "    expectation = gx.expectations.ExpectColumnValuesToNotBeNull(\n",
    "        column=col_name\n",
    "    )\n",
    "\n",
    "    validation_results = batch.validate(expectation)\n",
    "    \n",
    "    if validation_results.success:\n",
    "        print(f\"✅ Nulidade da coluna '{col_name}' validada com sucesso!\")\n",
    "    else:\n",
    "        \n",
    "        print(f\"❌ FALHA na validação de nulidade da coluna '{col_name}'\")\n",
    "        print(validation_results)\n",
    "        \n",
    "        # raise Exception(f\"Falha na validação de nulos para a coluna {col_name}: {validation_results}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Validando Duplicados na Chave Primária ---\")\n",
    "pk_column_name = \"id_envio\"\n",
    "expectation = gx.expectations.ExpectColumnValuesToBeUnique(\n",
    "    column=pk_column_name\n",
    ")\n",
    "\n",
    "validation_results = batch.validate(expectation)\n",
    "\n",
    "if validation_results.success:\n",
    "    print(f\"✅ Unicidade da coluna '{pk_column_name}' validada com sucesso!\")\n",
    "else:\n",
    "    print(f\"❌ FALHA: Foram encontrados valores duplicados na coluna '{pk_column_name}'\")\n",
    "    print(validation_results)\n",
    "    # raise Exception(f\"Falha na validação de unicidade para a coluna {pk_column_name}: {validation_results}\")\n",
    "\n",
    "print(\"\\nValidação concluída.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1329b9-a880-4391-bba3-b829a3d67585",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inclue coluna de controle de ingestao"
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"DtIngest\", F.lit(data.strftime(\"%Y-%m-%d %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3e80a44-ab9b-4271-ad4c-908185f42af7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Salva os dados na Camada bronze"
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"grao.bronze.grain_shipping\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5222636134385191,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ntb_ingest_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d82342-2712-438d-aa76-b4ec206f1f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8605acb-df9c-464e-b7aa-601404acf774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, date_format, col\n",
    "from pyspark.sql.types import IntegerType, FloatType, TimestampType\n",
    "\n",
    "df = spark.table(\"grao.bronze.grain_shipping\")\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"id_envio\", col(\"id_envio\").cast(IntegerType()))\n",
    "    .withColumn(\"ligacoes_do_cliente\", col(\"ligações_do_cliente\").cast(IntegerType()))\n",
    "    .withColumn(\"avaliacao_do_cliente\", col(\"avaliação_do_cliente\").cast(IntegerType()))\n",
    "    .withColumn(\"preco\", col(\"preço\").cast(FloatType()))\n",
    "    .withColumn(\"qtd_itens\", col(\"qtd_itens\").cast(IntegerType()))\n",
    "    .withColumn(\"desconto\", col(\"desconto\").cast(FloatType()))\n",
    "    .withColumn(\"peso_g\", col(\"peso_g\").cast(FloatType()))\n",
    "    .withColumn(\"avaliacaoEntrega\", col(\"avaliacaoEntrega\").cast(IntegerType()))\n",
    "    .withColumn(\"dtIngest\", col(\"dtIngest\").cast(TimestampType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85afb143-4f2e-427c-907d-1da8fb69c12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sequence, explode, date_format, col, year, month, dayofmonth, expr\n",
    "\n",
    "# Define o range de datas desejado\n",
    "data_inicio = \"2020-01-01\"\n",
    "data_fim = \"2030-12-31\"\n",
    "\n",
    "# Cria DataFrame com sequência de datas\n",
    "df_calendario = (\n",
    "    spark\n",
    "    .range(1)\n",
    "    .select(sequence(expr(f\"to_date('{data_inicio}')\"), expr(f\"to_date('{data_fim}')\")).alias(\"datas\"))\n",
    "    .select(explode(col(\"datas\")).alias(\"data\"))\n",
    ")\n",
    "\n",
    "# Adiciona colunas formatadas\n",
    "df_calendario = (\n",
    "    df_calendario\n",
    "    .withColumn(\"data_completa\", date_format(col(\"data\"), \"EEEE, d 'de' MMMM 'de' yyyy\"))\n",
    "    .withColumn(\"ano\", year(col(\"data\")))\n",
    "    .withColumn(\"mes\", month(col(\"data\")))\n",
    "    .withColumn(\"dia\", dayofmonth(col(\"data\")))\n",
    "    .withColumn(\"data_ddMMyyyy\", date_format(col(\"data\"), \"dd-MM-yyyy\"))\n",
    ")\n",
    "\n",
    "# Salva como tabela bronze.calendario\n",
    "df_calendario.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze.calendario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd40af29-e12a-48cf-9154-3b85704b1904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, FloatType, TimestampType\n",
    "\n",
    "# Mapeia meses em português para número\n",
    "meses_map = {\n",
    "    \"janeiro\": \"01\", \"fevereiro\": \"02\", \"março\": \"03\", \"abril\": \"04\",\n",
    "    \"maio\": \"05\", \"junho\": \"06\", \"julho\": \"07\", \"agosto\": \"08\",\n",
    "    \"setembro\": \"09\", \"outubro\": \"10\", \"novembro\": \"11\", \"dezembro\": \"12\"\n",
    "}\n",
    "\n",
    "# 1. Remove dia da semana e vírgula\n",
    "df = df.withColumn(\"data_limpa\", F.regexp_replace(\"dataEnvio\", \"^[^,]+,\\\\s*\", \"\"))\n",
    "\n",
    "# 2. Extrai dia (número antes do \" de\")\n",
    "df = df.withColumn(\"dia\", F.regexp_extract(\"data_limpa\", r\"(\\d{1,2}) de\", 1))\n",
    "\n",
    "# 3. Extrai mês (palavra entre \"de \" e \" de\" )\n",
    "df = df.withColumn(\"mes_texto\", F.regexp_extract(\"data_limpa\", r\"de ([a-zç]+) de\", 1))\n",
    "\n",
    "# 4. Extrai ano (número no final)\n",
    "df = df.withColumn(\"ano\", F.regexp_extract(\"data_limpa\", r\"(\\d{4})$\", 1))\n",
    "\n",
    "# 5. Cria coluna mês numérico usando mapping via expr CASE WHEN\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "case_expr = \"CASE \"\n",
    "for pt, num in meses_map.items():\n",
    "    case_expr += f\"WHEN mes_texto = '{pt}' THEN '{num}' \"\n",
    "case_expr += \"END as mes_num\"\n",
    "\n",
    "df = df.withColumn(\"mes_num\", expr(case_expr))\n",
    "\n",
    "# 6. Concatena para formar yyyy-MM-dd (formato ISO)\n",
    "df = df.withColumn(\n",
    "    \"dataEnvio\",\n",
    "    F.to_date(F.concat_ws(\"-\", \"ano\", \"mes_num\", \"dia\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Cast outras colunas normalmente\n",
    "df = df.withColumn(\"id_envio\", F.col(\"id_envio\").cast(IntegerType())) \\\n",
    "    .withColumn(\"ligacoes_do_cliente\", F.col(\"ligações_do_cliente\").cast(IntegerType())) \\\n",
    "    .withColumn(\"avaliacao_do_cliente\", F.col(\"avaliação_do_cliente\").cast(IntegerType())) \\\n",
    "    .withColumn(\"preco\", F.col(\"preço\").cast(FloatType())) \\\n",
    "    .withColumn(\"qtd_itens\", F.col(\"qtd_itens\").cast(IntegerType())) \\\n",
    "    .withColumn(\"desconto\", F.col(\"desconto\").cast(FloatType())) \\\n",
    "    .withColumn(\"peso_g\", F.col(\"peso_g\").cast(FloatType())) \\\n",
    "    .withColumn(\"avaliacaoEntrega\", F.col(\"avaliacaoEntrega\").cast(IntegerType())) \\\n",
    "    .withColumn(\"dtIngest\", F.col(\"dtIngest\").cast(TimestampType()))\n",
    "\n",
    "# Remove colunas temporárias\n",
    "df = df.drop(\"data_limpa\", \"dia\", \"mes_texto\", \"ano\", \"mes_num\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72866783-63b4-4d93-8977-b0832f34ca65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"grao.bronze.grain_shipping\")\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"id_envio\", F.col(\"id_envio\").cast(IntegerType()))\n",
    "    .withColumn(\"ligacoes_do_cliente\", F.col(\"ligações_do_cliente\").cast(IntegerType()))\n",
    "    .withColumn(\"avaliacao_do_cliente\", F.col(\"avaliação_do_cliente\").cast(IntegerType()))\n",
    "    .withColumn(\"preco\", F.col(\"preço\").cast(FloatType()))\n",
    "    .withColumn(\"qtd_itens\", F.col(\"qtd_itens\").cast(IntegerType()))\n",
    "    .withColumn(\"desconto\", F.col(\"desconto\").cast(FloatType()))\n",
    "    .withColumn(\"peso_g\", F.col(\"peso_g\").cast(FloatType()))\n",
    "    .withColumn(\"dataEnvio\", F.to_date(F.date_format(F.col(\"dataEnvio\"), \"EEEE, d 'de' MMMM 'de' yyyy\")))\n",
    "    .withColumn(\"dataEntrega\", F.to_date(F.date_format(F.col(\"dataEntrega\"), \"EEEE, d 'de' MMMM 'de' yyyy\")))\n",
    "    .withColumn(\"avaliacaoEntrega\", F.col(\"avaliacaoEntrega\").cast(IntegerType()))\n",
    "    .withColumn(\"dtIngest\", F.col(\"dtIngest\").cast(TimestampType()))\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f5f82cc-ca34-4046-a063-fa88883b547d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edbb8a72-ff1d-4382-943f-6042389966e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ntb_ingest_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
